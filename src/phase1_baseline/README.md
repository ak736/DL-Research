# Phase 1: Baseline LoRA Implementation

**Goal:** Build baseline LoRA that demonstrates catastrophic forgetting problem.

---

## ğŸ“‹ What This Phase Does

This phase creates a **standard LoRA baseline** that:

1. âœ… Trains on cardiology data
2. âŒ Forgets general medical knowledge (THE PROBLEM!)
3. ğŸ“Š Provides baseline results for comparison

**This is the problem Phase 2 (Fisher-LoRA) will solve!**

---

## ğŸ—ï¸ Project Structure

```
phase1_baseline/
â”œâ”€â”€ model.py              # LoRA model wrapper
â”œâ”€â”€ train_baseline.py     # Main training script â­
â”œâ”€â”€ evaluate.py           # Evaluation functions
â””â”€â”€ README.md            # This file

outputs/
â””â”€â”€ phase1_baseline/
    â”œâ”€â”€ models/
    â”‚   â””â”€â”€ baseline_lora.pt
    â””â”€â”€ baseline_results.json
```

---

## ğŸš€ Quick Start

### 1. Setup (One-time)

```bash
# Install dependencies
pip install -r requirements.txt

# Prepare data (if not done already)
python quick_setup.py --num-samples 100
```

### 2. Test Pipeline (Recommended First)

```bash
# Quick test with minimal data (~2-3 minutes)
python test_train_baseline.py
```

**Expected output:**

- Model loads âœ…
- Training runs âœ…
- Evaluation works âœ…
- Results saved âœ…

### 3. Run Full Baseline

```bash
# Full training (~10-15 minutes on CPU)
python src/phase1_baseline/train_baseline.py
```

---

## ğŸ“Š Expected Results

### What Success Looks Like

```
BEFORE Training:
  General Medicine: ~37% (random guessing with GPT-2)
  Cardiology:       ~10%

AFTER Training:
  General Medicine: ~30% (WORSE! âŒ Forgot!)
  Cardiology:       ~60% (Better! âœ… Learned!)

Forgetting: 7% drop
```

**This proves catastrophic forgetting exists!**

---

## ğŸ“ Output Files

### `baseline_results.json`

```json
{
  "before_training": {
    "general_medicine": {
      "accuracy": 0.37,
      "ece": 0.5,
      "brier_score": 1.07
    },
    "cardiology": {
      "accuracy": 0.1,
      "ece": 0.73,
      "brier_score": 1.44
    }
  },
  "after_training": {
    "general_medicine": {
      "accuracy": 0.3, // DROPPED!
      "ece": 0.52,
      "brier_score": 1.15
    },
    "cardiology": {
      "accuracy": 0.6, // IMPROVED!
      "ece": 0.45,
      "brier_score": 0.82
    }
  },
  "forgetting": 0.07, // 7% forgetting
  "cardiology_improvement": 0.5
}
```

### `models/baseline_lora.pt`

- Trained LoRA adapters
- Can be loaded for testing
- Used as baseline for comparison

---

## ğŸ”§ Configuration

Edit `train_baseline.py` to change:

```python
config = {
    'model_name': 'gpt2',        # Model to use
    'lora_r': 8,                 # LoRA rank
    'lora_alpha': 16,            # LoRA alpha
    'learning_rate': 2e-4,       # Learning rate
    'num_epochs': 3,             # Training epochs
    'batch_size': 4,             # Batch size
}
```

**For faster testing:** Reduce `num_epochs` to 1, `batch_size` to 2.

---

## ğŸ› Troubleshooting

### Issue: Out of Memory

**Solution:** Reduce batch size

```python
'batch_size': 2  # or 1
```

### Issue: Training too slow

**Solution:** Use fewer samples

```python
cardiology_train = cardiology_train[:50]  # Just 50 samples
```

### Issue: Model not improving

**Normal!** GPT-2 is not a medical model. Low accuracy is expected.
Switch to BioMistral in Week 1 Day 6 for real results.

---

## âœ… Phase 1 Checklist

Week 1 deliverables:

- [ ] Data loader works (`data_loader.py`)
- [ ] Metrics calculator works (`metrics.py`)
- [ ] Model initializes (`model.py`)
- [ ] Evaluation runs (`evaluate.py`)
- [ ] Training completes (`train_baseline.py`)
- [ ] Results show forgetting âŒ
- [ ] Results show cardiology learning âœ…
- [ ] Code documented and clean
- [ ] Ready for Sarghi testing (Week 2)

---

## ğŸ”„ Handoff to Sarghi (Week 2)

**What Sarghi needs:**

1. All code in `src/phase1_baseline/`
2. Results in `outputs/phase1_baseline/`
3. This README

**What Sarghi will do:**

- Run `test_train_baseline.py` to verify
- Run full training 3 times (check consistency)
- Document results
- Report any bugs

---

## ğŸ“š Code Overview

### `model.py`

- `LoRABaselineModel` class
- Wraps base model + LoRA adapters
- Methods: `predict()`, `predict_batch()`, `save_model()`

### `evaluate.py`

- `evaluate_on_dataset()` - Evaluate on any dataset
- `evaluate_general_and_cardiology()` - Both domains
- `compare_before_after()` - Calculate forgetting

### `train_baseline.py` â­

- `BaselineTrainer` class
- Main training loop
- Saves results and models

---

## ğŸ¯ Next Steps

After Phase 1 complete:

**Week 2:** Sarghi tests baseline

**Week 3-7:**

- Aniket â†’ Phase 2 (Fisher-LoRA)
- Sarghi â†’ Phase 3 (Other baselines)

**Week 8+:** Integration and experiments

---

## ğŸ’¡ Tips

1. **Start small:** Use `test_train_baseline.py` first
2. **Check outputs:** Verify `baseline_results.json` makes sense
3. **GPT-2 is fine:** Low accuracy is expected for testing
4. **Document issues:** Note any problems for team meeting

---

## ğŸ“ Questions?

- Check team documentation in `docs/`
- Review `Plan.docx` for overall strategy
- Ask in team meeting

**Phase 1 Goal:** Prove forgetting exists. âœ…
**Phase 2 Goal:** Fix it with Fisher-LoRA! ğŸš€
